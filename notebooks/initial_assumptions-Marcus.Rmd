---
title: "initial_assumptions"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(car)
library(lmtest)
library(sandwich)
library(stargazer)

```


```{r}
friends <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends.csv')
friends_emotions <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends_emotions.csv')
friends_info <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends_info.csv')
```

### friends_emotions is missing some rows (full scenes, actually & everything in seasons 5-10)
```{r}
print(nrow(friends))
print("^friends -- we will NOT be using this dataset. Although it seems to have every line from all 10 seasons of the show, it fails to include any information on emotion or views")
nrow(friends_emotions)
print("^friends_emotions -- we WILL be using this dataset to determine how prevalent emotions were in each episode!
      1 issue: it doesn't have every scene & it doesn't have every episode! Only seasons 1-4. Scenes tbd")
nrow(friends_info)
print("^friends_info -- we WILL be using this dataset to determine when episodes aired and how many views each episode got")
```

## Creating Intermediate DataFrames

### DF1: emot_by_ep has the count of each emotion in each episode

```{r}
friends_emotions
```

```{r -- friend_emotion dataset, error = FALSE}
#Make DF emot_by_ep that has the count of each emotion in each episode
emot_by_ep <- friends_emotions %>%
  group_by(season, episode, emotion) %>%
  summarise(num_utterances = n()) %>%
  spread(emotion, num_utterances) 
## Replace any NA Values with 0s
emot_by_ep[is.na(emot_by_ep)] <- 0
## Create the tot_emot column that has the total # of emotional utterances in an episode
emot_by_ep <- emot_by_ep %>%
  mutate(tot_emot = Joyful + Mad + Neutral + Peaceful + Powerful + Sad + Scared)
  
emot_by_ep
```

```{r}
## Add new columns to emot_by_ep that track the % of each emotion (not just raw count)
emot_by_ep$Joyful_percent <- (emot_by_ep$Joyful/emot_by_ep$tot_emot) * 100
emot_by_ep$Mad_percent <- (emot_by_ep$Mad/emot_by_ep$tot_emot) * 100
emot_by_ep$Neutral_percent <- (emot_by_ep$Neutral/emot_by_ep$tot_emot) * 100
emot_by_ep$Peaceful_percent <- (emot_by_ep$Peaceful/emot_by_ep$tot_emot) * 100
emot_by_ep$Powerful_percent <- (emot_by_ep$Powerful/emot_by_ep$tot_emot) * 100
emot_by_ep$Sad_percent <- (emot_by_ep$Sad/emot_by_ep$tot_emot) * 100
emot_by_ep$Scared_percent <- (emot_by_ep$Scared/emot_by_ep$tot_emot) * 100
emot_by_ep
```


### DF2: friends_info has the 'metadata' on each episode like views, air_date, director etc.

```{r -- friends_info 2}
## add column that gives the raw episode index #
friends_info$episode_index <- 1:nrow(friends_info)
friends_info
```

```{r}
## Inspect 2 datasets that we'll merge together
emot_by_ep 
friends_info
print("emot_by_ep")
print(nrow(emot_by_ep))
print (colnames(emot_by_ep))
print("              ")
print("friends_info")
print(nrow(friends_info))
colnames(friends_info)
```


## Making the Final Datasets 
#### 1. episode_by_emotions -- includes the pure counts and %s of emotions in each episode + the metadata about each episode
#### Note: when we build the model, we'll likely need to be selective about which columns to use to avoid perfect colinearity!

```{r -- emot_by_ep & friends_info > episode_by_emotions}
episode_by_emotions <- merge(x = emot_by_ep, y = friends_info, by.x = c("season", "episode"), by.y = c("season", "episode")) %>%
  arrange(episode_index)
## remove the 2 rows for "The One After The Super Bowl"
episode_by_emotions <- episode_by_emotions[-c(36,37), ]
episode_by_emotions
  
```
```{r -- episode_by_emotions}
show(episode_by_emotions)
```


## Add Calendar_season column to our dataset







## Models

```{r}

model_base <- lm(us_views_millions ~ Neutral_percent, data=episode_by_emotions)
model_emotions_only <- lm(us_views_millions ~ Joyful_percent + Mad_percent + Peaceful_percent + Powerful_percent + Sad_percent + Scared_percent, data=episode_by_emotions)
model_emotions_controls <- lm(us_views_millions ~ Joyful_percent + Mad_percent + Peaceful_percent + Powerful_percent + Sad_percent + Scared_percent + season + air_date,  data=episode_by_emotions)

stargazer(model_base,
          model_emotions_only,
          model_emotions_controls,
          type = "text", star.cutoffs = c(.05, .01, .001), omit = "Constant")
```


## Test Assumptions

#### IID
Given that we are only looking at data from one show, the data is not IID if this model is used to generalize the effect of emotions on viewership in other shows. Instead, we can use it to explain the causal relationship between the two variables in Friends and try to generalize to future seasons. In this case, the variables are time dependent but the autocorrelation it causes will be addressed by controlling for time fixed effects. Otherwise given the size and sampling of the data, it is sufficiently IID to create a useful regression model. 

#### Unique BLP exists

The variables here are not linear combinations of each other; there is no perfect colinearity as seen in the lack of perfect correlation in the matrix below. (Note we excluded one of the emotion_percents to avoid perfect colinearity). We check to make sure every covariance in our matrix is finite and the only entries with covariance == 1.0 are the ones between the exact same variables-- This is what we find! This suggests there is a unique BLP because there's no heavy tails in our dataset and there's no perfect colinearity.

```{r}
cor(episode_by_emotions[c(1, 11:17, 22)])

```


Notes on time fixed effects and implementing them: https://www.econometrics-with-r.org/10-4-regression-with-time-fixed-effects.html 

#### Linear Conditional Expectation
The below plots reveal that all of our models meet the Linear Conditional Expectation requirement to use the Central Limit Theorem. To satisfy the requirement there needs to be a straight line of best fit when the residuals are plotted against the fitted values. This means that if the expectation was taken conditional on any x it would be the same value. Although all of the lines are not exactly straight issues do not arise due to the number of data points (97), and the small degree of curvature present. 

```{r}

residualPlots(model_base)
residualPlots(model_emotions_only)
residualPlots(model_emotions_controls)

```

#### Homoskedastic Errors
When the model only includes the percentage of neutral emotions there is homoscedasticity (uniform variance) present however, this is not the case for the other two models. The Scale-Location plots (the square root of the standardized residuals plotted against the fitted values) reveal this as the lines for the last two models are not straight. One theory could be that the controls bias the models by introducing an outlier in the bottom left that creates this error. 

```{r}

plot(model_base)

plot(model_emotions_only)

plot(model_emotions_controls)


```

#### Normally Distributed Errors
To satisfy this assumptions the error from the models must have a mean of 0, normal distribution, and constant variance. To test this we can either plot a histogram of the residuals and see the distribution or create a qqplot of the Theoretical Quantiles against the Residuals. In the qqPlot we are looking for a blue box that is close to the line. We currently do not meet this assumption, we could try transforming some variables or  ommitting outliers. 
```{r}
res_base <- residuals(model_base)
res_base_control <- residuals(model_base_controls)
res_all_emotions <- residuals(model_all_emotions)

qqPlot(res_base, xlab = "Theoretical Quantiles", ylab="Residuals")
qqPlot(res_base_control, xlab = "Theoretical Quantiles", ylab="Residuals")
qqPlot(res_all_emotions, xlab = "Theoretical Quantiles", ylab="Residuals")

hist(res_base)
hist(res_base_control)
hist(res_all_emotions)

```

