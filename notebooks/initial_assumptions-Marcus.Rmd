---
title: "initial_assumptions"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(car)
library(lmtest)
library(sandwich)
library(stargazer)
```


```{r}
friends <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends.csv')
friends_emotions <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends_emotions.csv')
friends_info <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends_info.csv')
```

### friends_emotions is missing some rows (full scenes, actually & everything in seasons 5-10)
```{r}
print(nrow(friends))
print("^friends -- we will NOT be using this dataset. Although it seems to have every line from all 10 seasons of the show, it fails to include any information on emotion or views")
nrow(friends_emotions)
print("^friends_emotions -- we WILL be using this dataset to determine how prevalent emotions were in each episode!
      1 issue: it doesn't have every scene & it doesn't have every episode! Only seasons 1-4. Scenes tbd")
nrow(friends_info)
print("^friends_info -- we WILL be using this dataset to determine when episodes aired and how many views each episode got")
```

## Creating Intermediate DataFrames

### DF1: emot_by_ep has the count of each emotion in each episode

```{r}
friends_emotions
```

```{r -- friend_emotion dataset, error = FALSE}
#Make DF emot_by_ep that has the count of each emotion in each episode
emot_by_ep <- friends_emotions %>%
  group_by(season, episode, emotion) %>%
  summarise(num_utterances = n()) %>%
  spread(emotion, num_utterances) 
## Replace any NA Values with 0s
emot_by_ep[is.na(emot_by_ep)] <- 0
## Create the tot_emot column that has the total # of emotional utterances in an episode
emot_by_ep <- emot_by_ep %>%
  mutate(tot_emot = Joyful + Mad + Neutral + Peaceful + Powerful + Sad + Scared)
  
emot_by_ep
```

```{r}
## Add new columns to emot_by_ep that track the % of each emotion (not just raw count)
emot_by_ep$Joyful_percent <- (emot_by_ep$Joyful/emot_by_ep$tot_emot) * 100
emot_by_ep$Mad_percent <- (emot_by_ep$Mad/emot_by_ep$tot_emot) * 100
emot_by_ep$Neutral_percent <- (emot_by_ep$Neutral/emot_by_ep$tot_emot) * 100
emot_by_ep$Peaceful_percent <- (emot_by_ep$Peaceful/emot_by_ep$tot_emot) * 100
emot_by_ep$Powerful_percent <- (emot_by_ep$Powerful/emot_by_ep$tot_emot) * 100
emot_by_ep$Sad_percent <- (emot_by_ep$Sad/emot_by_ep$tot_emot) * 100
emot_by_ep$Scared_percent <- (emot_by_ep$Scared/emot_by_ep$tot_emot) * 100
emot_by_ep
```


### DF2: friends_info has the 'metadata' on each episode like views, air_date, director etc.

```{r -- friends_info 2}
## add column that gives the raw episode index #
friends_info$episode_index <- 1:nrow(friends_info)
friends_info
```

```{r}
## Inspect 2 datasets that we'll merge together
emot_by_ep 
friends_info
print("emot_by_ep")
print(nrow(emot_by_ep))
print (colnames(emot_by_ep))
print("              ")
print("friends_info")
print(nrow(friends_info))
colnames(friends_info)
```


## Making the Final Datasets 
#### 1. episode_by_emotions -- includes the pure counts and %s of emotions in each episode + the metadata about each episode
#### Note: when we build the model, we'll likely need to be selective about which columns to use to avoid perfect colinearity!

```{r -- emot_by_ep & friends_info > episode_by_emotions}
episode_by_emotions <- merge(x = emot_by_ep, y = friends_info, by.x = c("season", "episode"), by.y = c("season", "episode")) %>%
  arrange(episode_index)
## remove the 2 rows for "The One After The Super Bowl"
episode_by_emotions <- episode_by_emotions[-c(36,37), ]
episode_by_emotions
  
```
```{r -- episode_by_emotions}
show(episode_by_emotions)
```
## Display scatterplots of each regression variable against US Views to check if we need transforms.

Doesn't look like we need any transforms. 

```{r}
ggplot(episode_by_emotions, aes(x = Neutral_percent, y = us_views_millions)) + geom_point()

ggplot(episode_by_emotions, aes(x = Joyful_percent, y = us_views_millions)) + geom_point()

ggplot(episode_by_emotions, aes(x = Mad_percent, y = us_views_millions)) + geom_point()

ggplot(episode_by_emotions, aes(x = Peaceful_percent, y = us_views_millions)) + geom_point()

ggplot(episode_by_emotions, aes(x = Powerful_percent, y = us_views_millions)) + geom_point()

ggplot(episode_by_emotions, aes(x = Sad_percent, y = us_views_millions)) + geom_point()

ggplot(episode_by_emotions, aes(x = Scared_percent, y = us_views_millions)) + geom_point()

```



## Add Calendar_season column to our dataset for time fixed effects.


```{r}

getSeason <- function(DATES) {
    WS <- as.Date("2012-12-15", format = "%Y-%m-%d") # Winter Solstice
    SE <- as.Date("2012-3-15",  format = "%Y-%m-%d") # Spring Equinox
    SS <- as.Date("2012-6-15",  format = "%Y-%m-%d") # Summer Solstice
    FE <- as.Date("2012-9-15",  format = "%Y-%m-%d") # Fall Equinox

    # Convert dates from any year to 2012 dates
    d <- as.Date(strftime(DATES, format="2012-%m-%d"))

    ifelse (d >= WS | d < SE, "Winter",
      ifelse (d >= SE & d < SS, "Spring",
        ifelse (d >= SS & d < FE, "Summer", "Fall")))
}

episode_by_emotions$calendar_season <- getSeason(as.Date(episode_by_emotions$air_date, format = "%Y-%m-%d"))

```


## Models

```{r}
model_base <- lm(us_views_millions ~ Neutral_percent, data=episode_by_emotions)
model_emotions_only <- lm(us_views_millions ~ Joyful_percent + Mad_percent + Peaceful_percent + Powerful_percent + Sad_percent + Scared_percent, data=episode_by_emotions)
model_emotions_controls <- lm(us_views_millions ~ Joyful_percent + Mad_percent + Peaceful_percent + Powerful_percent + Sad_percent + Scared_percent + factor(season) + factor(calendar_season),  data=episode_by_emotions)
stargazer(model_base,
          model_emotions_only,
          coeftest(model_emotions_only, vcovHC(model_emotions_only, type = "HC3")),
          model_emotions_controls,
          type = "text", star.cutoffs = c(.05, .01, .001), omit = "Constant")

summary(model_emotions_only)
# Do Bonferroni correction
```


## Test Assumptions

#### IID
Given that we are only looking at data from one show, the data is not IID if this model is used to generalize the effect of emotions on viewership in other shows. Instead, we can use it to explain the causal relationship between the two variables in Friends and try to generalize to future seasons. In this case, the variables are time-dependent but the autocorrelation it causes will be addressed by controlling for time using the calendar season of the air date. Otherwise given the size and sampling of the data, it is sufficiently IID to create a useful regression model. 

#### Unique BLP exists

The variables here are not linear combinations of each other; there is no perfect colinearity as seen in the lack of perfect correlation in the matrix below. (Note we excluded one of the emotion_percents to avoid perfect colinearity). We check to make sure every covariance in our matrix is finite and the only entries with covariance == 1.0 are the ones between the same variables-- This is what we find! This suggests there is a unique BLP because there are no heavy tails in our dataset and there's no perfect colinearity.

```{r}
cor(episode_by_emotions[c(1, 11:17, 22)])
```


#### Linear Conditional Expectation
The below plots reveal that all of our models meet the Linear Conditional Expectation requirement to use the Central Limit Theorem. To satisfy the requirement there needs to be a straight line of best fit where the residuals are plotted against the fitted values. This means that if the expectation was taken conditional on any x it would be the same value. Although all of the lines are not exactly straight issues do not arise due to the number of data points (95), and the small degree of curvature present. 

```{r}
residualPlots(model_base)
residualPlots(model_emotions_only)
residualPlots(model_emotions_controls)
```

#### Homoskedastic Errors
The base model with a Neutral percent and the third model with 6 of the 7 emotions and controls appear to be relatively homoskedastic. This can be seen through the Scale-Location plots (the square root of the standardized residuals plotted against the fitted values) as the lines are relatively straight. However, this is not the case for the second model as the line of best fit appears to be almost parabolic in shape. To adjust for the heteroskedasticity in the second model (model_emotions_only) we decided to run a coeftest on the model with standard robust errors. Although standard robust errors increase the variance and can potentially bias the results we did not find issues with it due to the high number of data points. This allows us to justify the homoskedastic errors assumption. 

```{r}

plot(model_base)


plot(model_emotions_only)


plot(model_emotions_controls)

```

#### Normally Distributed Errors
To satisfy this assumption the error from the models must have a mean of 0, normal distribution, and constant variance. To test this we can either plot a histogram of the residuals and see the distribution or create a Q-Q plot of the Theoretical Quantiles against the Residuals. In the Q-Q plot, we are looking for a blue box that is close to the line and includes all data points. We initially did not meet this assumption, but after omitting two outliers this issue was fixed and the errors seem relatively normally distributed. 
```{r}
res_base <- residuals(model_base)
res_base_control <- residuals(model_emotions_only)
res_all_emotions <- residuals(model_emotions_controls)
qqPlot(res_base, xlab = "Theoretical Quantiles", ylab="Residuals")
qqPlot(res_base_control, xlab = "Theoretical Quantiles", ylab="Residuals")
qqPlot(res_all_emotions, xlab = "Theoretical Quantiles", ylab="Residuals")
hist(res_base)
hist(res_base_control)
hist(res_all_emotions)
```
